{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore images sizes\n",
    "\n",
    "We don't want to downscale or upscale input images too much as it can hurt model's performance. It seems reasonable if we take images' width and height around the third quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_image_sizes(image_fnames):\n",
    "    img_ws = []\n",
    "    img_hs = []\n",
    "    \n",
    "    for img_fname in image_fnames:\n",
    "        img = cv2.imread(img_fname)\n",
    "        img_hs.append(img.shape[0])\n",
    "        img_ws.append(img.shape[1])\n",
    "    \n",
    "    print(f\"Image width: min {np.min(img_ws)}, max {np.max(img_ws)}, quartiles {np.quantile(img_ws, [0.25, 0.5, 0.75])}\")\n",
    "    print(f\"Image height: min {np.min(img_hs)}, max {np.max(img_hs)}, quartiles {np.quantile(img_hs, [0.25, 0.5, 0.75])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image width: min 59, max 882, quartiles [170. 228. 301.]\n",
      "Image height: min 13, max 191, quartiles [36. 49. 65.]\n"
     ]
    }
   ],
   "source": [
    "analyse_image_sizes(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/*/img/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data fields\n",
    "\n",
    "We are interested in parsed plates. There are two main fields that seem to contain this information, one is called \"description\" and one is \"predicted\" under \"moderation\" section.\n",
    "\n",
    "First, we check if the are equal all the time. This could be the easiest option, but it's not the case. \n",
    "\n",
    "Second, we test these fields against image file names in validation and test sets that seem to also match plates' numbers. This is also not the case.\n",
    "\n",
    "Since we don't have a reliable source of true labels, we can stick to the \"description\" field that has the highest matching rate. We must remember that noisy labels can hurt models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_labels_match(label_fnames):\n",
    "    matches = []\n",
    "    for label_fname in label_fnames:\n",
    "        with open(label_fname, \"rt\") as f:\n",
    "            label = json.load(f)\n",
    "            match = int( label[\"description\"] == label[\"moderation\"][\"predicted\"] )\n",
    "            matches.append(match)\n",
    "    \n",
    "    all_match = ( np.sum(matches) == len(matches) )\n",
    "    print(f\"All match: {all_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All match: False\n"
     ]
    }
   ],
   "source": [
    "assert_labels_match(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/*/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_description_fnam_match(label_fnames):\n",
    "    matches = []\n",
    "    for label_fname in label_fnames:\n",
    "        with open(label_fname, \"rt\") as f:\n",
    "            label = json.load(f)\n",
    "            match = int( label[\"description\"] == os.path.basename(label_fname).split(\".\")[0] )\n",
    "            matches.append(match)\n",
    "    \n",
    "    all_match = ( np.sum(matches) == len(matches) )\n",
    "    print(f\"All match: {all_match}, match ratio {np.sum(matches)/len(matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All match: False, match ratio 0.8813080339119903\n"
     ]
    }
   ],
   "source": [
    "assert_description_fnam_match(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/val/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All match: False, match ratio 0.8168717047451669\n"
     ]
    }
   ],
   "source": [
    "assert_description_fnam_match(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/test/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_moderation_fnam_match(label_fnames):\n",
    "    matches = []\n",
    "    for label_fname in label_fnames:\n",
    "        with open(label_fname, \"rt\") as f:\n",
    "            label = json.load(f)\n",
    "            match = int( label[\"moderation\"][\"predicted\"] == os.path.basename(label_fname).split(\".\")[0] )\n",
    "            matches.append(match)\n",
    "    \n",
    "    all_match = ( np.sum(matches) == len(matches) )\n",
    "    print(f\"All match: {all_match}, match ratio {np.sum(matches)/len(matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All match: False, match ratio 0.8772708922083166\n"
     ]
    }
   ],
   "source": [
    "assert_moderation_fnam_match(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/val/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All match: False, match ratio 0.81195079086116\n"
     ]
    }
   ],
   "source": [
    "assert_moderation_fnam_match(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/test/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking label length\n",
    "\n",
    "Finally, we're interested in lengthes of our labels as it will determine our models' architecture. So we take the maximum length of our label field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_description_sizes(label_fnames):\n",
    "    lengths = []\n",
    "    for label_fname in label_fnames:\n",
    "        with open(label_fname, \"rt\") as f:\n",
    "            label = json.load(f)\n",
    "            lengths.append(len(label[\"description\"]))\n",
    "    \n",
    "    print(f\"Description length: min {np.min(lengths)}, max {np.max(lengths)}, quartiles {np.quantile(lengths, [0.25, 0.5, 0.75])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description length: min 8, max 9, quartiles [8. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "analyse_description_sizes(glob.glob(\"autoriaNumberplateOcrRu-2020-10-12/*/ann/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
